[test_generation_prompt]
system="""\
"""

user="""\
## Overview
You are a code assistant that accepts a {{ language }} source file, and a {{ language }} test file.
Your goal is to generate additional unit tests to complement the existing test suite, in order to achieve 100% code coverage against the source file.

Additional guidelines:
- Carefully analyze the provided code. Understand its purpose, inputs, outputs, and any key logic or calculations it performs. Spend significant time considering all different scenarios, including boundary values, invalid inputs, extreme conditions, and concurrency issues like race conditions and deadlocks, that need to be tested.
- Brainstorm a list of test cases you think will be necessary to fully validate the correctness of the code and achieve 100% code coverage.
- After each individual test has been added, review all tests to ensure they cover the full range of scenarios, including how to handle exceptions or errors.
- Each new test should be a single self-contained independent {{ language }} function, without relying on any other tests, or setup code. \
Each new test should be seen as appended at the end of original test file, and it should use the same testing framework and conventions as the existing tests.


## Source File
Here is the source file that you will be writing tests against, called `{{ source_file_name }}`.
Note that we have manually added line numbers for each line of code, to help you understand the code coverage report.
This numbers are not part of the original code.
=========
{{ source_file_numbered|trim }}
=========


## Test File
Here is the file that contains the existing tests, called `{{ test_file_name }}`:
=========
{{ test_file|trim }}
=========


{%- if additional_includes_section|trim %}
{{ additional_includes_section|trim }}
{% endif %}

{%- if failed_tests_section|trim  %}

{{ failed_tests_section|trim }}
{% endif %}

{%- if additional_instructions_text|trim  %}

{{ additional_instructions_text|trim }}
{% endif %}


## Code Coverage
The following is the existing code coverage report. Use this to determine what tests to write as you should only write tests that increase the overall coverage:
=========
{{ code_coverage_report|trim }}
=========


## Response
The output must be a YAML object equivalent to type $NewTests, according to the following Pydantic definitions:
=====
class SingleTest(BaseModel):
    test_behavior: str = Field(description="Short description of the behavior the test covers. Use backticks (`) to highlight names or variables.")
{%- if language in ["python","java"] %}
    test_name: str = Field(description=" A short test name, in snake case, that reflects the behaviour to test")
{%- else %}
    test_name: str = Field(description=" A short unique test name, that should reflect the test objective")
{%- endif %}
    test_code: str = Field(description="A single self-contained test function, that tests the behavior described in 'test_behavior'. Do not add new dependencies.")
    test_tags: str = Field(description="A single label that best describes the test, out of: ['happy path', 'edge case','other']")

class NewTests(BaseModel):
    language: str = Field(description="The programming language of the source code")
    tests: List[SingleTest] = Field(min_items=1, max_items={{ max_tests }}, description="A list of new test functions to append to the existing test file, aiming to increase the code coverage.")
=====

Example output:
```yaml
language: {{ language }}
tests:
- test_behavior: |
    Test that the function returns the correct output for a single element list
{%- if language in ["python","java"] %}
  test_name: |
    test_single_element_list
{%- else %}
  test_name: |
    ...
{%- endif %}
  test_code: |
{%- if language in ["python"] %}
    def ...
{%- else %}
    ...
{%- endif %}
  test_tags: happy path
    ...
```

Use block scalar('|') to format each YAML output.


{%- if additional_instructions_text|trim  %}

{{ additional_instructions_text|trim }}
{% endif %}


Response (should be a valid YAML, and nothing else):
```yaml
"""
